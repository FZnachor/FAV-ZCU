**Stacionární iterační metody pro soustavy lineárních algebraických rovnic. Jacobiho a Gaussova-Seidelova metoda, SOR metoda. Nutná a postačující podmínka konvergence iterační metody, postačující podmínka konvergence iterační metody.**

### Stacionární iterační metody SLAR

+ používány pro řídké matice
+ pro plnou matici raději GEM nebo LU-rozklad
+ **stacionární** = nalezené rovnice se nemění, vhodné pro výpočty na PC

- soustavu přepíšeme:
	- $Ax = b \to x = Hx+g$
- iterační formule
	- $x^{(k+1)} = H \cdot x^{(k)} + g$
	- $H$ rozhoduje o kvalitě metody
+ zvolíme počáteční aproximaci $x^{(0)}$
+ zastavovací podmínka $\Vert x^{(k)}-x^{(k-1)}\Vert < \epsilon$

**Jacobiho metoda**
- matice musí být **regulární** - soustava má jedno řešení
	- pokud je ostře diagonálně dominantní, tak konverguje vždy
- z $i$-té rovnice (řádky) vyjádříme složku $x_{i}$ vektoru $x$
	- $i$-tá rovnice ... $a_{i1}x_{1} + a_{i2}x_{2} + \dots a_{in}x_{n} = b_{i}$
- iterační formule
	- $\displaystyle x_{i}^{(k+1)} = \frac{1}{a_{ii}}\left( b_{i} - \sum^n_{j=1; \, j\neq i} a_{ij}x_{j}^{(k)} \right)$ pro $a_{ii} \neq 0$
	- od $b_{i}$ odečítáme sumu všech $a_{ij}$ přes všechna $j$ kde $j\neq i$

**Gaussova-Seidelova metoda**
- stejný princip jako u Jacobihovy metody, ale pokud při výpočtu $(k+1)$-iterace již známe $(k+1)$ iteraci u některých složek, tak ji použijeme
- iterační formule
	- $\displaystyle x_{i}^{(k+1)} = \frac{1}{a_{ii}}\left( b_{i} - \sum^{i-1}_{j=1} a_{ij}x_{j}^{(k+1)} - \sum^{n}_{j=i+1} a_{ij} x_{j}^{(k) }\right)$
	- od $b_{i}$ odečítáme sumu $(k+1)$-tých iterací u $j < i-1$ a sumu $k$-tých iterací u $j > i+1$

**SOR metoda**
- princip
	- vychází z Gauss-Seidelovy metody
	- vyjádříme $(k+1)$-iteraci pomocí $k$-té iterace a změny ... $x_{i}^{(k+1)} = x_{i}^{(k)} + r_{i}^{(k)}$
	- idea: k urychlení nepřičteme změnu $r_{i}^{(k)}$ ale její násobek $\omega\cdot r_{i}^{(k)}$
- iterační formule
	- $\displaystyle x_{i}^{(k+1)} = \omega\cdot x_{i,GS}^{(k+1)} + (1-\omega)x_{i}^{(k)}$
	- lineární kombinace $(k+1)$-iterace GS-metody a $k$-té iterace metody SOR
- volba **relaxačního parametru** $\omega$
	- musíme si zvolit parametr $\omega \in (0,2)$
	- tento parametr může metodu zhoršit či vylepšit oproti GS
	- vzorec, který (ne vždy) dokáže vypočítat optimální $\omega$
		- $\displaystyle\omega_\text{opt} = \frac{2}{1 + \sqrt{ 1- \rho(H_{J}) }}$
		- $\rho(H_{J})$ ... spektrální poloměr Jacobiho matice $H$

### Konvergence iteračních metod

$\displaystyle\lim_{ x \to \infty } x^{(k)} = x^*$

**Nutná a postačující podmínka konvergence**
- $\rho(H) < 1$ $\Longleftrightarrow$ metoda konverguje pro libovolné $x_{0} \in R \Longleftrightarrow$ úloha je stabilní
	- $\rho(H) = \max|\lambda_{i}(H)|$ ... spektrální poloměr matice $H$
		- maximální vlastní číslo matice $H$ v absolutní hodnotě
- čím blíž bude spektrální poloměr 1, tím bude metoda pomalejší
	- snaha, dostat ho co nejvíce k 0

**Postačující podmínka konvergence**
- $\Vert H\Vert \leq q < 1 \implies$ metoda konverguje při libovolné volbě $x_{0}$
	- Euklidovská maticová norma ... $\displaystyle\Vert A\Vert = \sqrt{ \sum_{i=1}^n\sum_{j=1}^n a^2_{ij} }$
+ podmínka pro konvergenci SOR
	- $\rho(H_{SOR}) \geq |\omega-1| \quad \forall \omega \in R$

Konvergenční věty
- podmínky pro $H$ jsou nepraktické, $H$ je těžko spočitatelná
- $A$ je ostře diagonálně dominantní $\implies$ konverguje Jacobiho i GS metoda pro libovolnou volbu $x_{0}$
- $A$ je symetrická a poz. definitní $\implies$ konverguje GS metoda
- $A$ je symetrická a poz. definitní, $0 < \omega < 2 \implies$ SOR metoda konverguje
