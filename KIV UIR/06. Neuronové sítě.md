# Neuronové sítě

**Neuron**
- mozková buňka, která má za úkol **sběr**, **zpracovávání** a **šíření signálů**
- mozek
	- $10^{11}$ neuronů, více než 20 typů, $10^{14}$ synapsí
	- 1ms - 10ms cyklus nosiče informace
		- signály = výkyvy elektrických potenciálů (se šumem)

**Počítačový model**
- matematický model neuronu (McCulloch & Pitts, 1943)
- při **spojení** do **neuronové sítě** mají schopnost **tolerovat šum** ve vstupu a **učit se**
- jednotky v neuronové síti (units)
	- propojeny vazbami (links)
	- vazba z jednotky $j$ do $i$ propaguje aktivaci $a_j$ jednotky $j$
	- každá vazba má číselnou váhu $W_{j,i}$ (síla + znaménko)
- účel aktivační funkce
	- jednotka má být aktivní ($\approx +1$) pro pozitivní příklady, jinak neaktivní $\approx 0$
	- aktivace musí být nelineární, jinak by celá síť byla lineární
- kombinacemi těchto jednotek do sítě můžeme implementovat libovolnou Booleovskou funkci

## Struktury neuronových sítí

**Sítě s předním vstupem**
- necyklické
- implementují funkce
- nemají vnitřní paměť
- **příklad**
	- síť 5-ti jednotek - 2 vstupní, 1 skrytá vrstva (2 jednotky), 1 výstupní jednotka
- = parametrizovaná nelineární funkce vstupu

**Rekurentní sítě**
- cyklické
- vlastní výstup si berou opět na vstup
- složitější a schopnější
- výstup má (zpožděný) vliv na aktivaci (= paměť)
- **Hopfieldovy sítě**
	- symetrické obousměrné vazby; fungují jako asociativní paměť
- **Boltzmannovy stroje**
	- pravděpodobnostní aktivační funkce

**Perceptron**
- nejjednodušší model neuronové sítě s učením s učitelem
	- pouze jedna vstupní jednotka
	- nízká vyjadřovací síla
	- pro složitější klasifikaci - více výstupních jednotek
	- dokáže reprezentovat hodně Bool. funkcí - AND, OR, NOT, ...
- výhody
	- existuje jednoduchý učící algoritmus pro libovolnou lineárně separabilní funkci
- učení perceptronu
	- upravování vah, aby se snížila chyba na trénovací sadě
	- učící pravidlo pro perceptron konverguje ke správné funkci pro libovolnou lineárně separabilní množinu dat
- limity jednoduchého perceptronu
	- algoritmus je konečný právě tehdy, když je množina (učící) lineárně separovatelná a pokud existuje řešení

**Vícevrstvé neuronové sítě**
- vrstvy jsou obvykle úplně propojené
- počet skrytých jednotek je obvykle volen experimentálně
- dostatečná vyjadřovací síla
	- s **jednou** skrytou vrstvou - všechny spojité funkce
	- se **dvěma** skrytými vrstvami - všechny funkce
	- těžko se ovšem pro konkrétní síť zjišťuje její prostor reprezentovatelných funkcí
- učení
	- pravidla pro úpravu vah
		- výstupní vrstva - stejné jako u perceptronu
		- skryté vrstvy - **zpětné šíření** (back-propagation) chyby z výstupní vrstvy
	- problémy učení
		- dosažení lokálního minima chyby
		- příliš pomalá konvergencce
		- přílišné upnutí na příklady -> neschopnost generalizovat
